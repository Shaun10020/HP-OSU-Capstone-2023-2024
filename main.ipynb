{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from config.args import get_arguments\n",
    "from config.config import features,labels,duplex_labels,train_test_ratio,label_extension,threshold, train_val_ratio, random\n",
    "from dataloader.load_data import SimplexDataset, DuplexDataset, InputSimplexDataset, InputDuplexDataset, load_dataloader\n",
    "from utils.load_json import load_results\n",
    "from utils.save_load_model import load\n",
    "from utils.convert import convertBinary\n",
    "from utils.metrics import binary_iou\n",
    "from utils.output_viz import print_output\n",
    "from model.UNet import UNet, CustomUNet\n",
    "from model.ENet import ENet\n",
    "from model.DeepLabV3 import CustomDeepLabV3\n",
    "from train.train import Train\n",
    "from train.test import Test\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataset):\n",
    "    if random:\n",
    "        train_set, test_set = random_split(dataset,train_test_ratio,torch.Generator())\n",
    "    else:\n",
    "        train_size = len(dataset)*train_test_ratio[0]\n",
    "        train_set = Subset(dataset,range(int(train_size)))\n",
    "        test_set = Subset(dataset,range(int(train_size),len(dataset)))\n",
    "    filepath = os.path.join(args.save_folder,f'''{args.model}-{args.dataset}.pt''')\n",
    "    if os.path.exists(filepath):\n",
    "        model = load(model,args)\n",
    "    train = Train(model,device,train_set,args)\n",
    "    train.run()\n",
    "    \n",
    "    model = load(model,args)\n",
    "    test = Test(model,device,test_set,int(args.batch))\n",
    "    test.run()\n",
    "    \n",
    "    train.save_plot()\n",
    "\n",
    "def test(model,dataset):\n",
    "    model = load(model,args)\n",
    "    test = Test(model,device,dataset,int(args.batch))\n",
    "    test.run()\n",
    "\n",
    "def inference(model):\n",
    "    if not os.path.exists(args.output_folder):\n",
    "        os.mkdir(args.output_folder)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if args.dataset == \"simplex\":\n",
    "        data = InputSimplexDataset(args)\n",
    "    elif args.dataset == \"duplex\":\n",
    "        data = InputDuplexDataset(args)\n",
    "    loader = DataLoader(data,batch_size = int(args.batch))\n",
    "    \n",
    "    json_name = \"\"\n",
    "    intermediate = []\n",
    "    for batch in loader:\n",
    "        input = batch[2].to(device)\n",
    "        outputs = model(input.float())\n",
    "        outputs = convertBinary(outputs)\n",
    "        for name,pn,output in zip(batch[0],batch[1],outputs):\n",
    "            if not json_name:\n",
    "                json_name = name\n",
    "            if json_name != name:\n",
    "                fp = open(os.path.join(args.output_folder,json_name,\"results.json\"),\"w\")\n",
    "                json.dump({\"intermediate_results\":intermediate},fp)\n",
    "                fp.close()\n",
    "                json_name = name\n",
    "                intermediate = []\n",
    "            pn = pn.item()\n",
    "            pgnum = (4-len(str(pn)))*\"0\" + str(pn)\n",
    "            json_instance = {\"pdf_filename\":name+\".pdf\",\"page_num\":pn,\"intermediate_dir\":\"intermediate_results/\"+pgnum}\n",
    "            path = os.path.join(args.output_folder,name)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            path = os.path.join(path,\"intermediate_results\")\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            if args.dataset == \"simplex\":\n",
    "                path = os.path.join(path,pgnum)\n",
    "                if not os.path.exists(path):\n",
    "                    os.mkdir(path)\n",
    "                for i, label in enumerate(labels):\n",
    "                    to_pil_image(output[i]).save(os.path.join(path,f'''{label}{label_extension}'''))\n",
    "                    json_instance[label] = json_instance[\"intermediate_dir\"]+\"/\"+str(label)+str(label_extension)\n",
    "                intermediate.append(json_instance)\n",
    "            \n",
    "            elif args.dataset == \"duplex\":\n",
    "                pgnum2 = (4-len(str(pn)))*\"0\" + str(pn+1)\n",
    "                json_instance2 = {\"pdf_filename\":name+\".pdf\",\"page_num\":pn+1,\"intermediate_dir\":\"intermediate_results/\"+pgnum2}\n",
    "                path2 = os.path.join(path,pgnum2)\n",
    "                path = os.path.join(path,pgnum)\n",
    "                if not os.path.exists(path):\n",
    "                    os.mkdir(path)\n",
    "                if not os.path.exists(path2):\n",
    "                    os.mkdir(path2)\n",
    "                cur = 0\n",
    "                for i, label in enumerate(labels):\n",
    "                    to_pil_image(output[cur + i]).save(os.path.join(path,f'''{label}{label_extension}'''))\n",
    "                    json_instance[label] = json_instance[\"intermediate_dir\"]+\"/\"+str(label)+str(label_extension)\n",
    "                cur += len(labels)\n",
    "                for i, label in enumerate(duplex_labels):\n",
    "                    to_pil_image(output[cur + i]).save(os.path.join(path,f'''{label}{label_extension}'''))\n",
    "                    json_instance[label] = json_instance[\"intermediate_dir\"]+\"/\"+str(label)+str(label_extension)\n",
    "                cur += len(duplex_labels)\n",
    "                for i, label in enumerate(labels):\n",
    "                    to_pil_image(output[cur + i]).save(os.path.join(path2,f'''{label}{label_extension}'''))\n",
    "                    json_instance2[label] = json_instance2[\"intermediate_dir\"]+\"/\"+str(label)+str(label_extension)\n",
    "                intermediate.append(json_instance)\n",
    "                intermediate.append(json_instance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class default:\n",
    "    def __init__(self):\n",
    "        self.input_folder = \"./data/cache/DP_a2200_xml_ff2c81d8ad6655f915cbaa558ee7bf9e878730a8\"\n",
    "        self.output_folder = \"./output\"\n",
    "        self.label_folder = \"./data/output/DP_a2200_xml_ff2c81d8ad6655f915cbaa558ee7bf9e878730a8\"\n",
    "        self.save_folder = \"./checkpoints\"\n",
    "        self.model = \"unet\"\n",
    "        self.mode = \"train\"\n",
    "        self.epoch = 200\n",
    "        self.lr = 1e-5\n",
    "        self.batch = 5\n",
    "        self.dataset = \"simplex\"\n",
    "        \n",
    "args = default()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device: cuda\n",
      "INFO: Preparing SimplexDataset...\n",
      "INFO: Finished preparing SimplexDataset\n",
      "INFO: Total 5093 samples\n",
      "INFO: Initializing UNet Model...\n",
      "INFO: Done initialize UNet Model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logging.info(f'''Using device: {device}''')\n",
    "\n",
    "n_input = len(features) if args.dataset == 'simplex' else 2*len(features)\n",
    "n_output = len(labels) if args.dataset == 'simplex' else 2*len(labels)+len(duplex_labels)\n",
    "if args.mode != 'inference':\n",
    "    pdf,algorithm,intermediate = load_results(args.label_folder)\n",
    "    if args.dataset =='simplex':\n",
    "        dataset = SimplexDataset(args.input_folder,args.label_folder,intermediate)\n",
    "    else:\n",
    "        dataset = DuplexDataset(args.input_folder,args.label_folder,intermediate)\n",
    "        \n",
    "if args.model == 'unet':\n",
    "    model = CustomUNet(n_input,n_output)\n",
    "elif args.model == 'enet':\n",
    "    model = ENet(n_input,n_output)\n",
    "elif args.model == 'deeplabv3':\n",
    "    model = CustomDeepLabV3(n_output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing training script...\n",
      "INFO: Preparing Dataloader...\n",
      "INFO: Done preparing Dataloader\n",
      "INFO: Done initialize training script\n",
      "  3%|â–Ž         | 23/917 [00:33<21:48,  1.46s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m IoU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     12\u001b[0m label_IoU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m tqdm(train\u001b[38;5;241m.\u001b[39mtrain_dataloader):\n\u001b[0;32m     14\u001b[0m     train\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(train\u001b[38;5;241m.\u001b[39mdevice), batch_data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(train\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:1192\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1190\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[1;32m-> 1192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(n \u001b[38;5;241m-\u001b[39m last_print_n)\n\u001b[0;32m   1193\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[0;32m   1194\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:1243\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:1348\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:1496\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:462\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    461\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 462\u001b[0m     fp_write(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m len_s, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m    463\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\std.py:456\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    455\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 456\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\tqdm\\utils.py:195\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\site-packages\\ipykernel\\iostream.py:578\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_timeout):\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\coding space\\network\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args.lr=5e-4\n",
    "train = Train(model,device,dataset,args)\n",
    "train.model.to(device)\n",
    "train.model.train()\n",
    "# batch = next(iter(train.train_dataloader)) \n",
    "# inputs, labels = batch[0].to(train.device), batch[1].to(train.device)\n",
    "# preds = train.model(inputs.float())\n",
    "\n",
    "for i in range(args.epoch):\n",
    "    epoch_loss = 0.0\n",
    "    IoU = 0.0\n",
    "    label_IoU = 0.0\n",
    "    for batch_data in tqdm(train.train_dataloader):\n",
    "        train.optim.zero_grad()\n",
    "        inputs, labels = batch_data[0].to(train.device), batch_data[1].to(train.device)\n",
    "        preds = train.model(inputs.float())\n",
    "        loss = train.criterion(preds,convertBinary(labels))\n",
    "        loss.backward()\n",
    "        train.optim.step()\n",
    "        epoch_loss += loss.item()\n",
    "        IoU += binary_iou(convertBinary(preds),convertBinary(labels))\n",
    "        label_IoU += binary_iou(labels,convertBinary(labels))\n",
    "    label_IoU /= len(train.train_dataloader)\n",
    "    train.train_IoU.append(IoU / len(train.train_dataloader))\n",
    "    train.epoch_losses.append(epoch_loss / len(train.train_dataloader))\n",
    "    logging.info(f'''Epoch [{i+1}/{int(train.args.epoch)}], Loss: {train.epoch_losses[-1]:.4f} Train IoU: {train.train_IoU[-1] * 100:.2f}%, Label IoU: {label_IoU * 100:.2f}%''')\n",
    "    print_output(0,preds,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
